{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T09:28:39.412693Z",
     "start_time": "2018-05-03T09:28:39.038695Z"
    }
   },
   "outputs": [],
   "source": [
    "! code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】\n",
    "http://neuro-educator.com/rl1/\n",
    "\n",
    "----------------------------------------------\n",
    "- ロードマップ：<span class=\"burk\">Q学習</span>　→　DQN　→　A3C → PPO →　HER\n",
    "- jupyterでgymの結果を表示するのは面倒なので、実行はcodeでする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コードの中で分からなかった事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy でビンの作成、データとビンの対応関係の取得\n",
    "https://qiita.com/shoz@github/items/86cbce27f0631cfde8c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.random.normal(0, 1, n)\n",
    "    > 正規分布の行列を作成する。0が中心で1が分散（大きいとばらつく）、nが出力個数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.linspace(clip_min, clip_max, num + 1)[1:-1])\n",
    "    > 線形に等間隔な数列を生成  \n",
    "clip_maxとclip_minをnumの数で等分割して、行列で返す  \n",
    "下の例では、1と11との間を10等分させ、はじめの値を抜いて返している  \n",
    "np.linspace(clip_min, clip_max, num + 1)[0:-1]であれば、1から始まる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T09:44:51.489972Z",
     "start_time": "2018-05-03T09:44:50.988896Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 100\n",
    "dist = np.random.normal(0, 1, n)\n",
    "num=10\n",
    "clip_min=-5\n",
    "clip_max=5\n",
    "bins=np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "plt.hist(dist, bins=bins) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.digitize(dist, bins)\n",
    "    > データに対応したビンの位置情報が格納された行列を取得  \n",
    "    distの各要素がbinsの何番目の要素にマッチしているかを得られる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T09:47:40.986078Z",
     "start_time": "2018-05-03T09:47:40.976079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bin_indice = np.digitize(dist, bins)\n",
    "bin_indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qテーブルの作成\n",
    "最小-1、最大+1の間をランダムに表を作成  \n",
    "- 行数：num_dizitized**num_state\n",
    "- 列数：num_action　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T11:01:03.169657Z",
     "start_time": "2018-05-03T11:01:03.141656Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_dizitized=6 #6分割\n",
    "num_state=4\n",
    "num_action=2 #env.action_space.n\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**num_state,num_action))\n",
    "df = pd.DataFrame(q_table)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cartPoleをＱ学習で学習\n",
    "\n",
    "\n",
    "1. 状態s：倒立振子の状態（State）は下記の4変数\n",
    "    - カート位置 -2.4～2.4\n",
    "    - カート速度 -3.0～3.0\n",
    "    - 棒の角度 　-41.8～41.8\n",
    "    - 棒の角速度　-2.0～2.0\n",
    "\n",
    "1. 行動a：実際にとることができる行動（Action）下記2通り→加速度を与える\n",
    "    - カートを右に押す\n",
    "    - カートを左に押す\n",
    "\n",
    "1. 報酬\n",
    "    - 失敗：棒が20.9度以上傾いたり、カート位置が±2.4以上移動すると失敗\n",
    "    - 成功：上記以外\n",
    "\n",
    "1. 目的\n",
    "    - 状態sに応じて、うまく行動aをとり、棒を立て続ける戦略（policy)を作る\n",
    "\n",
    "\n",
    "今回の場合200stepの間、立て続ければ成功  \n",
    "つまり  \n",
    "a_t=A(s_t)　※時刻tにおいて状態sのときに最適な行動aを返す関数A（policy)  \n",
    "を求めることがゴールとなります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:12:35.808621Z",
     "start_time": "2018-05-06T06:12:35.654566Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import gym  #倒立振子(cartpole)の実行環境\n",
    "from gym import wrappers  #gymの画像保存\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q関数を離散化して定義する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:12:36.638689Z",
     "start_time": "2018-05-06T06:12:36.623676Z"
    }
   },
   "outputs": [],
   "source": [
    "# 観測した状態を離散値にデジタル変換する\n",
    "def bins(clip_min, clip_max, num):\n",
    "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    " \n",
    "# 各値を離散値に変換\n",
    "def digitize_state(observation):\n",
    "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "    digitized = [\n",
    "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
    "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
    "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
    "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
    "    ]\n",
    "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行動a(t)を求める関数\n",
    "次の状態s(t+1)で右に動かすべきか、左に動かすべきか、Q関数の大きい方を選びます。  \n",
    "ただし、徐々に最適行動のみをとる、ε-greedy法にします。  \n",
    "※ε-greedy法：基本的には報酬が最大となる行動を選択しますが、ときおりランダムな行動をとります。  \n",
    "> 広く答えを探さなくなるジレンマに陥るため（探索と利用のジレンマ）  \n",
    "人間も時々冒険して学ぶ。人によって違うのは、このパラメータ（下記のepsilon）の違い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:12:37.487430Z",
     "start_time": "2018-05-06T06:12:37.478430Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_action(next_state, episode):\n",
    "           #徐々に最適行動のみをとる、ε-greedy法\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span class=\"burk\">Qテーブルを更新する関数</span> ←ここが肝\n",
    "Q関数を更新するメソッドを定義  \n",
    "エージェントは状態をQテーブルと見合わせて、どちらが有効かを判断する  \n",
    "※DQNではこの表がニューラルネットワークになる\n",
    "\n",
    "Qテーブル\n",
    "> 状態（State）の変数は4個で、これを6分割（6^4の1296状態で定義）  \n",
    "　→　例）カート速度 -3.0～3.0なら[-3.0 , -1.8 , -0.6 , 0.6 , 1.8 , 3.0]で分割  \n",
    "Q関数は状態（State）×action（2通り）で、[1296×2]の行列（表）で表される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:12:38.319135Z",
     "start_time": "2018-05-06T06:12:38.310135Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    gamma = 0.99\n",
    "    alpha = 0.5\n",
    "    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
    "            alpha * (reward + gamma * next_Max_Q)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン関数開始 パラメータ設定\n",
    "はじめに各パラメータを定義します。  \n",
    "また状態を離散値にして、[1296×2]の行列（表）形式のQ関数を作成します。（-1から1の間で初期値はランダムに作成する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:12:39.644661Z",
     "start_time": "2018-05-06T06:12:39.615658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "max_number_of_steps = 200  #1試行のstep数\n",
    "num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n",
    "num_episodes = 2000  #総試行回数\n",
    "goal_average_reward = 195  #この報酬を超えると学習終了（中心への制御なし）\n",
    "# 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成\n",
    "num_dizitized = 6  #分割数\n",
    "q_table = np.random.uniform(\n",
    "    low=-1, high=1, size=(num_dizitized**4, env.action_space.n))\n",
    " \n",
    "total_reward_vec = np.zeros(num_consecutive_iterations)  #ここに各試行の報酬を格納する\n",
    "final_x = np.zeros((num_episodes, 1))  #ここに学習後、各試行のt=200でのｘの位置を格納する\n",
    "islearned = 0  #学習が終わったフラグ\n",
    "isrender = 0  #描画フラグ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メインルーチン\n",
    "試行数のfor文と、各時間ステップのfor文のネストになっています。  \n",
    "状態s(t)でa(t)を実行し、観測状態s(t+1)を求めます。  \n",
    "そのときの棒が立っているかどうかで報酬r(t)を決定します。  \n",
    "報酬は、195ステップ立たずに終了したら-200の罰則の報酬を与えます。  \n",
    "こけずに立っていたら、+1の報酬を与えます。  \n",
    "その後、Q関数を更新し、次の行動a(t+1)を求め、状態s(t)を更新します。  \n",
    "最後は各ステップごとの情報と、試行終わりの情報を出力し、学習終了条件を満たしているか判定します。  \n",
    "以上のコードを実行すると、だいたい800試行で学習が収束し、棒がうまく立ちます。  \n",
    "上記コードを実行した結果をgifで示します。  \n",
    "40度以上傾くと終了します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- observation　：観測データ＝Stateなので、4つの状態array([-0.02029021, -0.03788568, -0.01769319,  0.00148244])\n",
    "- state　:observationをqテーブルの行番号へ変換\n",
    "- action　：カートの移動方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observation, reward, done, inf＝状態,報酬←このリワードは使わない,ゲームオーバかどうか（True or False),インフォメーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:12:41.709131Z",
     "start_time": "2018-05-06T06:12:41.694140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:13:11.743237Z",
     "start_time": "2018-05-06T06:12:42.511949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0Episode finished after25time steps total_reward_mean0.0\n",
      "1Episode finished after23time steps total_reward_mean-1.76\n",
      "2Episode finished after14time steps total_reward_mean-3.54\n",
      "3Episode finished after24time steps total_reward_mean-5.41\n",
      "4Episode finished after11time steps total_reward_mean-7.18\n",
      "5Episode finished after20time steps total_reward_mean-9.08\n",
      "6Episode finished after10time steps total_reward_mean-10.89\n",
      "7Episode finished after15time steps total_reward_mean-12.8\n",
      "8Episode finished after22time steps total_reward_mean-14.66\n",
      "9Episode finished after10time steps total_reward_mean-16.45\n",
      "10Episode finished after13time steps total_reward_mean-18.36\n",
      "11Episode finished after27time steps total_reward_mean-20.24\n",
      "12Episode finished after19time steps total_reward_mean-21.98\n",
      "13Episode finished after23time steps total_reward_mean-23.8\n",
      "14Episode finished after13time steps total_reward_mean-25.58\n",
      "15Episode finished after19time steps total_reward_mean-27.46\n",
      "16Episode finished after82time steps total_reward_mean-29.28\n",
      "17Episode finished after82time steps total_reward_mean-30.47\n",
      "18Episode finished after13time steps total_reward_mean-31.66\n",
      "19Episode finished after18time steps total_reward_mean-33.54\n",
      "20Episode finished after23time steps total_reward_mean-35.37\n",
      "21Episode finished after200time steps total_reward_mean-37.15\n",
      "22Episode finished after200time steps total_reward_mean-35.15\n",
      "23Episode finished after200time steps total_reward_mean-33.15\n",
      "24Episode finished after200time steps total_reward_mean-31.15\n",
      "25Episode finished after138time steps total_reward_mean-29.15\n",
      "26Episode finished after175time steps total_reward_mean-29.78\n",
      "27Episode finished after171time steps total_reward_mean-30.04\n",
      "28Episode finished after141time steps total_reward_mean-30.34\n",
      "29Episode finished after200time steps total_reward_mean-30.94\n",
      "30Episode finished after200time steps total_reward_mean-28.94\n",
      "31Episode finished after164time steps total_reward_mean-26.94\n",
      "32Episode finished after107time steps total_reward_mean-27.31\n",
      "33Episode finished after107time steps total_reward_mean-28.25\n",
      "34Episode finished after48time steps total_reward_mean-29.19\n",
      "35Episode finished after200time steps total_reward_mean-30.72\n",
      "36Episode finished after200time steps total_reward_mean-28.72\n",
      "37Episode finished after200time steps total_reward_mean-26.72\n",
      "38Episode finished after16time steps total_reward_mean-24.72\n",
      "39Episode finished after139time steps total_reward_mean-26.57\n",
      "40Episode finished after200time steps total_reward_mean-27.19\n",
      "41Episode finished after200time steps total_reward_mean-25.19\n",
      "42Episode finished after127time steps total_reward_mean-23.19\n",
      "43Episode finished after124time steps total_reward_mean-23.93\n",
      "44Episode finished after127time steps total_reward_mean-24.7\n",
      "45Episode finished after100time steps total_reward_mean-25.44\n",
      "46Episode finished after140time steps total_reward_mean-26.45\n",
      "47Episode finished after178time steps total_reward_mean-27.06\n",
      "48Episode finished after148time steps total_reward_mean-27.29\n",
      "49Episode finished after122time steps total_reward_mean-27.82\n",
      "50Episode finished after177time steps total_reward_mean-28.61\n",
      "51Episode finished after32time steps total_reward_mean-28.85\n",
      "52Episode finished after134time steps total_reward_mean-30.54\n",
      "53Episode finished after35time steps total_reward_mean-31.21\n",
      "54Episode finished after126time steps total_reward_mean-32.87\n",
      "55Episode finished after136time steps total_reward_mean-33.62\n",
      "56Episode finished after24time steps total_reward_mean-34.27\n",
      "57Episode finished after133time steps total_reward_mean-36.04\n",
      "58Episode finished after200time steps total_reward_mean-36.72\n",
      "59Episode finished after120time steps total_reward_mean-34.72\n",
      "60Episode finished after174time steps total_reward_mean-35.53\n",
      "61Episode finished after67time steps total_reward_mean-35.8\n",
      "62Episode finished after70time steps total_reward_mean-37.14\n",
      "63Episode finished after84time steps total_reward_mean-38.45\n",
      "64Episode finished after78time steps total_reward_mean-39.62\n",
      "65Episode finished after187time steps total_reward_mean-40.85\n",
      "66Episode finished after129time steps total_reward_mean-40.99\n",
      "67Episode finished after194time steps total_reward_mean-41.71\n",
      "68Episode finished after200time steps total_reward_mean-41.78\n",
      "69Episode finished after200time steps total_reward_mean-39.78\n",
      "70Episode finished after200time steps total_reward_mean-37.78\n",
      "71Episode finished after185time steps total_reward_mean-35.78\n",
      "72Episode finished after200time steps total_reward_mean-35.94\n",
      "73Episode finished after200time steps total_reward_mean-33.94\n",
      "74Episode finished after200time steps total_reward_mean-31.94\n",
      "75Episode finished after178time steps total_reward_mean-29.94\n",
      "76Episode finished after141time steps total_reward_mean-30.17\n",
      "77Episode finished after160time steps total_reward_mean-30.77\n",
      "78Episode finished after165time steps total_reward_mean-31.18\n",
      "79Episode finished after166time steps total_reward_mean-31.54\n",
      "80Episode finished after160time steps total_reward_mean-31.89\n",
      "81Episode finished after95time steps total_reward_mean-32.3\n",
      "82Episode finished after176time steps total_reward_mean-33.36\n",
      "83Episode finished after184time steps total_reward_mean-33.61\n",
      "84Episode finished after135time steps total_reward_mean-33.78\n",
      "85Episode finished after163time steps total_reward_mean-34.44\n",
      "86Episode finished after200time steps total_reward_mean-34.82\n",
      "87Episode finished after163time steps total_reward_mean-32.82\n",
      "88Episode finished after166time steps total_reward_mean-33.2\n",
      "89Episode finished after200time steps total_reward_mean-33.55\n",
      "90Episode finished after82time steps total_reward_mean-31.55\n",
      "91Episode finished after88time steps total_reward_mean-32.74\n",
      "92Episode finished after154time steps total_reward_mean-33.87\n",
      "93Episode finished after87time steps total_reward_mean-34.34\n",
      "94Episode finished after102time steps total_reward_mean-35.48\n",
      "95Episode finished after200time steps total_reward_mean-36.47\n",
      "96Episode finished after165time steps total_reward_mean-34.47\n",
      "97Episode finished after162time steps total_reward_mean-34.83\n",
      "98Episode finished after200time steps total_reward_mean-35.22\n",
      "99Episode finished after160time steps total_reward_mean-33.22\n",
      "100Episode finished after200time steps total_reward_mean-33.63\n",
      "101Episode finished after200time steps total_reward_mean-29.87\n",
      "102Episode finished after198time steps total_reward_mean-26.09\n",
      "103Episode finished after200time steps total_reward_mean-22.24\n",
      "104Episode finished after200time steps total_reward_mean-18.47\n",
      "105Episode finished after198time steps total_reward_mean-14.57\n",
      "106Episode finished after200time steps total_reward_mean-10.78\n",
      "107Episode finished after183time steps total_reward_mean-6.87\n",
      "108Episode finished after200time steps total_reward_mean-5.19\n",
      "109Episode finished after200time steps total_reward_mean-1.4\n",
      "110Episode finished after184time steps total_reward_mean2.51\n",
      "111Episode finished after200time steps total_reward_mean4.22\n",
      "112Episode finished after200time steps total_reward_mean7.96\n",
      "113Episode finished after200time steps total_reward_mean11.78\n",
      "114Episode finished after197time steps total_reward_mean15.56\n",
      "115Episode finished after178time steps total_reward_mean19.41\n",
      "116Episode finished after143time steps total_reward_mean21.0\n",
      "117Episode finished after200time steps total_reward_mean21.61\n",
      "118Episode finished after200time steps total_reward_mean24.8\n",
      "119Episode finished after47time steps total_reward_mean28.68\n",
      "120Episode finished after200time steps total_reward_mean28.97\n",
      "121Episode finished after99time steps total_reward_mean32.75\n",
      "122Episode finished after151time steps total_reward_mean29.73\n",
      "123Episode finished after149time steps total_reward_mean27.23\n",
      "124Episode finished after200time steps total_reward_mean24.71\n",
      "125Episode finished after200time steps total_reward_mean24.71\n",
      "126Episode finished after198time steps total_reward_mean27.34\n",
      "127Episode finished after145time steps total_reward_mean29.58\n",
      "128Episode finished after200time steps total_reward_mean29.32\n",
      "129Episode finished after178time steps total_reward_mean31.92\n",
      "130Episode finished after146time steps total_reward_mean29.69\n",
      "131Episode finished after200time steps total_reward_mean27.14\n",
      "132Episode finished after200time steps total_reward_mean29.51\n",
      "133Episode finished after126time steps total_reward_mean32.45\n",
      "134Episode finished after131time steps total_reward_mean32.64\n",
      "135Episode finished after176time steps total_reward_mean33.47\n",
      "136Episode finished after171time steps total_reward_mean31.22\n",
      "137Episode finished after200time steps total_reward_mean28.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138Episode finished after200time steps total_reward_mean28.92\n",
      "139Episode finished after200time steps total_reward_mean32.77\n",
      "140Episode finished after200time steps total_reward_mean35.39\n",
      "141Episode finished after200time steps total_reward_mean35.39\n",
      "142Episode finished after200time steps total_reward_mean35.39\n",
      "143Episode finished after200time steps total_reward_mean38.13\n",
      "144Episode finished after200time steps total_reward_mean40.9\n",
      "145Episode finished after200time steps total_reward_mean43.64\n",
      "146Episode finished after200time steps total_reward_mean46.65\n",
      "147Episode finished after200time steps total_reward_mean49.26\n",
      "148Episode finished after200time steps total_reward_mean51.49\n",
      "149Episode finished after200time steps total_reward_mean54.02\n",
      "150Episode finished after200time steps total_reward_mean56.81\n",
      "151Episode finished after200time steps total_reward_mean59.05\n",
      "152Episode finished after200time steps total_reward_mean62.74\n",
      "153Episode finished after200time steps total_reward_mean65.41\n",
      "154Episode finished after200time steps total_reward_mean69.07\n",
      "155Episode finished after200time steps total_reward_mean71.82\n",
      "156Episode finished after200time steps total_reward_mean74.47\n",
      "157Episode finished after200time steps total_reward_mean78.24\n",
      "158Episode finished after200time steps total_reward_mean80.92\n",
      "159Episode finished after200time steps total_reward_mean80.92\n",
      "160Episode finished after200time steps total_reward_mean83.73\n",
      "161Episode finished after200time steps total_reward_mean86.0\n",
      "162Episode finish"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10107 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for episode in range(num_episodes):  #試行数分繰り返す\n",
    "    # 環境の初期化\n",
    "    observation = env.reset()\n",
    "    state = digitize_state(observation)\n",
    "    action = np.argmax(q_table[state])\n",
    "    episode_reward = 0\n",
    " \n",
    "    for t in range(max_number_of_steps):  #1試行のループ\n",
    "        if islearned == 1:  #学習終了したらcartPoleを描画する\n",
    "            isrender=1\n",
    "#             env.render()\n",
    "            time.sleep(0.1)\n",
    "            print (observation[0])  #カートのx位置を出力\n",
    "\n",
    "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 報酬を設定し与える\n",
    "        if done: #ゲームオーバ　True or Flase　で、　True（ゲームオーバ：こけたの場合）\n",
    "            if t < 195:#195回連続でこけなかった場合\n",
    "                reward = -200  #こけたら罰則\n",
    "            else:\n",
    "                reward = 1  #立ったまま終了時は罰則はなし\n",
    "        else:\n",
    "            reward = 1  #各ステップで立ってたら報酬追加→ゲームオーバーになっていない：こけてない\n",
    " \n",
    "        episode_reward += reward  #報酬を追加\n",
    " \n",
    "        # 離散状態s_{t+1}を求め、Q関数を更新する\n",
    "        next_state = digitize_state(observation)  #t+1での観測状態を、離散値に変換\n",
    "        q_table = update_Qtable(q_table, state, action, reward, next_state) #qテーブルを更新 1ステップごとに\n",
    "        \n",
    "        #  次の行動a_{t+1}を求める \n",
    "        action = get_action(next_state, episode)    # a_{t+1} \n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        #終了時の処理\n",
    "        if done:#ゲームオーバ　True or Flase　で、　True（ゲームオーバ：こけたの場合）\n",
    "            print( str(episode) + 'Episode finished after' + str(t+1)+'time steps' +\n",
    "                ' total_reward_mean'+str(total_reward_vec.mean()) ) # total_reward_vec.mean()は全エピソードの報酬の平均\n",
    "            \n",
    "            total_reward_vec = np.hstack((total_reward_vec[1:],#np.hstackは配列の連結\n",
    "                                          episode_reward))  #報酬を記録\n",
    "#             print(total_reward_vec)\n",
    "            if islearned == 1:  #学習終わってたら最終のx座標を格納\n",
    "                final_x[episode, 0] = observation[0]\n",
    "            break\n",
    "\n",
    "    if (total_reward_vec.mean() >=goal_average_reward):  # 直近の100エピソードが規定報酬以上であれば成功\n",
    "        print('Episode %d train agent successfuly!' % episode)\n",
    "        islearned = 1\n",
    "        np.savetxt('learned_Q_table.csv',q_table, delimiter=\",\") #Qtableの保存する場合\n",
    "        # if isrender == 0:\n",
    "            # env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "            # isrender = 1\n",
    "    #10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
    "    # if episode>10:\n",
    "    #     islearned = 1\n",
    "\n",
    "    if islearned==1 and isrender==1:\n",
    "        break\n",
    "\n",
    "if islearned:\n",
    "    np.savetxt('final_x.csv', final_x, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:14:24.563913Z",
     "start_time": "2018-05-06T06:14:24.559912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:14:33.188185Z",
     "start_time": "2018-05-06T06:14:33.184185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:14:38.077216Z",
     "start_time": "2018-05-06T06:14:38.072215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T06:15:30.652709Z",
     "start_time": "2018-05-06T06:15:30.648708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
