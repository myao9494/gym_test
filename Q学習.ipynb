{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T09:28:39.412693Z",
     "start_time": "2018-05-03T09:28:39.038695Z"
    }
   },
   "outputs": [],
   "source": [
    "! code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPoleでQ学習（Q-learning）を実装・解説【Phythonで強化学習：第1回】\n",
    "http://neuro-educator.com/rl1/\n",
    "\n",
    "----------------------------------------------\n",
    "- ロードマップ：<span class=\"burk\">Q学習</span>　→　DQN　→　A3C → PPO →　HER\n",
    "- jupyterでgymの結果を表示するのは面倒なので、実行はcodeでする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コードの中で分からなかった事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy でビンの作成、データとビンの対応関係の取得\n",
    "https://qiita.com/shoz@github/items/86cbce27f0631cfde8c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.random.normal(0, 1, n)\n",
    "    > 正規分布の行列を作成する。0が中心で1が分散（大きいとばらつく）、nが出力個数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.linspace(clip_min, clip_max, num + 1)[1:-1])\n",
    "    > 線形に等間隔な数列を生成  \n",
    "clip_maxとclip_minをnumの数で等分割して、行列で返す  \n",
    "下の例では、1と11との間を10等分させ、はじめの値を抜いて返している  \n",
    "np.linspace(clip_min, clip_max, num + 1)[0:-1]であれば、1から始まる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T09:44:51.489972Z",
     "start_time": "2018-05-03T09:44:50.988896Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 100\n",
    "dist = np.random.normal(0, 1, n)\n",
    "num=10\n",
    "clip_min=-5\n",
    "clip_max=5\n",
    "bins=np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    "plt.hist(dist, bins=bins) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.digitize(dist, bins)\n",
    "    > データに対応したビンの位置情報が格納された行列を取得  \n",
    "    distの各要素がbinsの何番目の要素にマッチしているかを得られる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T09:47:40.986078Z",
     "start_time": "2018-05-03T09:47:40.976079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bin_indice = np.digitize(dist, bins)\n",
    "bin_indice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qテーブルの作成\n",
    "最小-1、最大+1の間をランダムに表を作成  \n",
    "- 行数：num_dizitized**num_state\n",
    "- 列数：num_action　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T11:01:03.169657Z",
     "start_time": "2018-05-03T11:01:03.141656Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "num_dizitized=6 #6分割\n",
    "num_state=4\n",
    "num_action=2 #env.action_space.n\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(num_dizitized**num_state,num_action))\n",
    "df = pd.DataFrame(q_table)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cartPoleをＱ学習で学習\n",
    "\n",
    "\n",
    "1. 状態s：倒立振子の状態（State）は下記の4変数\n",
    "    - カート位置 -2.4～2.4\n",
    "    - カート速度 -3.0～3.0\n",
    "    - 棒の角度 　-41.8～41.8\n",
    "    - 棒の角速度　-2.0～2.0\n",
    "\n",
    "1. 行動a：実際にとることができる行動（Action）下記2通り→加速度を与える\n",
    "    - カートを右に押す\n",
    "    - カートを左に押す\n",
    "\n",
    "1. 報酬\n",
    "    - 失敗：棒が20.9度以上傾いたり、カート位置が±2.4以上移動すると失敗\n",
    "    - 成功：上記以外\n",
    "\n",
    "1. 目的\n",
    "    - 状態sに応じて、うまく行動aをとり、棒を立て続ける戦略（policy)を作る\n",
    "\n",
    "\n",
    "今回の場合200stepの間、立て続ければ成功  \n",
    "つまり  \n",
    "a_t=A(s_t)　※時刻tにおいて状態sのときに最適な行動aを返す関数A（policy)  \n",
    "を求めることがゴールとなります。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T04:05:34.600786Z",
     "start_time": "2018-05-06T04:05:34.450314Z"
    }
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import gym  #倒立振子(cartpole)の実行環境\n",
    "from gym import wrappers  #gymの画像保存\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q関数を離散化して定義する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T04:05:35.233248Z",
     "start_time": "2018-05-06T04:05:35.217249Z"
    }
   },
   "outputs": [],
   "source": [
    "# 観測した状態を離散値にデジタル変換する\n",
    "def bins(clip_min, clip_max, num):\n",
    "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
    " \n",
    "# 各値を離散値に変換\n",
    "def digitize_state(observation):\n",
    "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "    digitized = [\n",
    "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
    "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
    "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
    "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
    "    ]\n",
    "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行動a(t)を求める関数\n",
    "次の状態s(t+1)で右に動かすべきか、左に動かすべきか、Q関数の大きい方を選びます。  \n",
    "ただし、徐々に最適行動のみをとる、ε-greedy法にします。  \n",
    "※ε-greedy法：基本的には報酬が最大となる行動を選択しますが、ときおりランダムな行動をとります。  \n",
    "> 広く答えを探さなくなるジレンマに陥るため（探索と利用のジレンマ）  \n",
    "人間も時々冒険して学ぶ。人によって違うのは、このパラメータ（下記のepsilon）の違い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T04:05:36.032345Z",
     "start_time": "2018-05-06T04:05:36.024345Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_action(next_state, episode):\n",
    "           #徐々に最適行動のみをとる、ε-greedy法\n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qテーブルを更新する関数\n",
    "Q関数を更新するメソッドを定義  \n",
    "エージェントは状態をQテーブルと見合わせて、どちらが有効かを判断する  \n",
    "※DQNではこの表がニューラルネットワークになる\n",
    "\n",
    "Qテーブル\n",
    "> 状態（State）の変数は4個で、これを6分割（6^4の1296状態で定義）  \n",
    "　→　例）カート速度 -3.0～3.0なら[-3.0 , -1.8 , -0.6 , 0.6 , 1.8 , 3.0]で分割  \n",
    "Q関数は状態（State）×action（2通り）で、[1296×2]の行列（表）で表される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T04:05:36.584492Z",
     "start_time": "2018-05-06T04:05:36.576497Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    gamma = 0.99\n",
    "    alpha = 0.5\n",
    "    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
    "            alpha * (reward + gamma * next_Max_Q)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン関数開始 パラメータ設定\n",
    "はじめに各パラメータを定義します。  \n",
    "また状態を離散値にして、[1296×2]の行列（表）形式のQ関数を作成します。（-1から1の間で初期値はランダムに作成する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T04:05:37.526478Z",
     "start_time": "2018-05-06T04:05:37.497479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "max_number_of_steps = 200  #1試行のstep数\n",
    "num_consecutive_iterations = 100  #学習完了評価に使用する平均試行回数\n",
    "num_episodes = 2000  #総試行回数\n",
    "goal_average_reward = 195  #この報酬を超えると学習終了（中心への制御なし）\n",
    "# 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成\n",
    "num_dizitized = 6  #分割数\n",
    "q_table = np.random.uniform(\n",
    "    low=-1, high=1, size=(num_dizitized**4, env.action_space.n))\n",
    " \n",
    "total_reward_vec = np.zeros(num_consecutive_iterations)  #ここに各試行の報酬を格納する\n",
    "final_x = np.zeros((num_episodes, 1))  #ここに学習後、各試行のt=200でのｘの位置を格納する\n",
    "islearned = 0  #学習が終わったフラグ\n",
    "isrender = 0  #描画フラグ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メインルーチン\n",
    "試行数のfor文と、各時間ステップのfor文のネストになっています。  \n",
    "状態s(t)でa(t)を実行し、観測状態s(t+1)を求めます。  \n",
    "そのときの棒が立っているかどうかで報酬r(t)を決定します。  \n",
    "報酬は、195ステップ立たずに終了したら-200の罰則の報酬を与えます。  \n",
    "こけずに立っていたら、+1の報酬を与えます。  \n",
    "その後、Q関数を更新し、次の行動a(t+1)を求め、状態s(t)を更新します。  \n",
    "最後は各ステップごとの情報と、試行終わりの情報を出力し、学習終了条件を満たしているか判定します。  \n",
    "以上のコードを実行すると、だいたい800試行で学習が収束し、棒がうまく立ちます。  \n",
    "上記コードを実行した結果をgifで示します。  \n",
    "40度以上傾くと終了します。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- observation　：観測データ＝Stateなので、4つの状態array([-0.02029021, -0.03788568, -0.01769319,  0.00148244])\n",
    "- state　:observationをqテーブルの行番号へ変換\n",
    "- action　：カートの移動方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observation, reward, done, inf＝状態,報酬←このリワードは使わない,ゲームオーバかどうか（True or False),インフォメーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T04:05:38.982360Z",
     "start_time": "2018-05-06T04:05:38.968361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-06T04:16:51.518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0Episode finished after61time steps total_reward_mean-66.23\n",
      "1Episode finished after80time steps total_reward_mean-67.63\n",
      "2Episode finished after106time steps total_reward_mean-68.84\n",
      "3Episode finished after87time steps total_reward_mean-69.79\n",
      "4Episode finished after13time steps total_reward_mean-70.93\n",
      "5Episode finished after126time steps total_reward_mean-72.81\n",
      "6Episode finished after55time steps total_reward_mean-73.56\n",
      "7Episode finished after90time steps total_reward_mean-75.02\n",
      "8Episode finished after39time steps total_reward_mean-76.13\n",
      "9Episode finished after94time steps total_reward_mean-77.75\n",
      "10Episode finished after87time steps total_reward_mean-78.82\n",
      "11Episode finished after75time steps total_reward_mean-79.96\n",
      "12Episode finished after37time steps total_reward_mean-81.22\n",
      "13Episode finished after120time steps total_reward_mean-82.86\n",
      "14Episode finished after184time steps total_reward_mean-83.67\n",
      "15Episode finished after200time steps total_reward_mean-83.84\n",
      "16Episode finished after200time steps total_reward_mean-81.84\n",
      "17Episode finished after10time steps total_reward_mean-79.84\n",
      "18Episode finished after200time steps total_reward_mean-81.75\n",
      "19Episode finished after200time steps total_reward_mean-79.75\n",
      "20Episode finished after35time steps total_reward_mean-77.75\n",
      "21Episode finished after155time steps total_reward_mean-79.41\n",
      "22Episode finished after144time steps total_reward_mean-79.87\n",
      "23Episode finished after154time steps total_reward_mean-80.44\n",
      "24Episode finished after10time steps total_reward_mean-80.91\n",
      "25Episode finished after200time steps total_reward_mean-82.82\n",
      "26Episode finished after158time steps total_reward_mean-80.82\n",
      "27Episode finished after38time steps total_reward_mean-81.25\n",
      "28Episode finished after135time steps total_reward_mean-82.88\n",
      "29Episode finished after50time steps total_reward_mean-83.54\n",
      "30Episode finished after45time steps total_reward_mean-85.05\n",
      "31Episode finished after145time steps total_reward_mean-86.61\n",
      "32Episode finished after82time steps total_reward_mean-87.17\n",
      "33Episode finished after120time steps total_reward_mean-88.36\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):  #試行数分繰り返す\n",
    "    # 環境の初期化\n",
    "    observation = env.reset()\n",
    "    state = digitize_state(observation)\n",
    "    action = np.argmax(q_table[state])\n",
    "    episode_reward = 0\n",
    " \n",
    "    for t in range(max_number_of_steps):  #1試行のループ\n",
    "        if islearned == 1:  #学習終了したらcartPoleを描画する\n",
    "#             env.render()\n",
    "            time.sleep(0.1)\n",
    "#             print (observation[0])  #カートのx位置を出力\n",
    " \n",
    "        # 行動a_tの実行により、s_{t+1}, r_{t}などを計算する\n",
    "        observation, reward, done, info = env.step(action)\n",
    " \n",
    "        # 報酬を設定し与える\n",
    "        if done: #ゲームオーバ　True or Flase　で、　True（ゲームオーバ：こけたの場合）\n",
    "            if t < 195:#195回連続でこけなかった場合\n",
    "                reward = -200  #こけたら罰則\n",
    "            else:\n",
    "                reward = 1  #立ったまま終了時は罰則はなし\n",
    "        else:\n",
    "            reward = 1  #各ステップで立ってたら報酬追加→ゲームオーバーになっていない：こけてない\n",
    " \n",
    "        episode_reward += reward  #報酬を追加\n",
    " \n",
    "        # 離散状態s_{t+1}を求め、Q関数を更新する\n",
    "        next_state = digitize_state(observation)  #t+1での観測状態を、離散値に変換\n",
    "        q_table = update_Qtable(q_table, state, action, reward, next_state) #qテーブルを更新\n",
    "        \n",
    "        #  次の行動a_{t+1}を求める \n",
    "        action = get_action(next_state, episode)    # a_{t+1} \n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        #終了時の処理\n",
    "        if done:#ゲームオーバ　True or Flase　で、　True（ゲームオーバ：こけたの場合）\n",
    "            print( str(episode) + 'Episode finished after' + str(t+1)+'time steps' +' total_reward_mean'+str(total_reward_vec.mean()) ) # total_reward_vec.mean()は全エピソードの報酬の平均\n",
    "            \n",
    "            total_reward_vec = np.hstack((total_reward_vec[1:],#np.hstackは配列の連結\n",
    "                                          episode_reward))  #報酬を記録\n",
    "#             print(total_reward_vec)\n",
    "            if islearned == 1:  #学習終わってたら最終のx座標を格納\n",
    "                final_x[episode, 0] = observation[0]\n",
    "            break\n",
    " \n",
    "    if (total_reward_vec.mean() >=\n",
    "            goal_average_reward):  # 直近の100エピソードが規定報酬以上であれば成功\n",
    "        print('Episode %d train agent successfuly!' % episode)\n",
    "        islearned = 1\n",
    "        #np.savetxt('learned_Q_table.csv',q_table, delimiter=\",\") #Qtableの保存する場合\n",
    "        if isrender == 0:\n",
    "            #env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "            isrender = 1\n",
    "    #10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
    "    if episode>50:\n",
    "        if isrender == 0:\n",
    "#             env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
    "            isrender = 1\n",
    "        elif islearned==1:\n",
    "            break\n",
    "\n",
    "# if islearned:\n",
    "#     np.savetxt('final_x.csv', final_x, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
